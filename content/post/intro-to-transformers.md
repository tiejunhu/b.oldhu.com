+++
draft = true
toc = false
date = "2023-06-05T16:24:22+08:00"
title = "Transformers in All Glory Details"
+++

https://www.youtube.com/watch?v=dqb4U-QzMbs

## Transformer Everywhere

视觉、自然语言、语音、翻译 全都可以用Transformer架构解决，而且现在大家都在这样做。

## Attention机制

最早引入的论文：2014年，Neural Machine Translation by Jointly Learning to Align and Traslate，在其中 attention 出现了三次。

2017年，Attention Is All You Need

